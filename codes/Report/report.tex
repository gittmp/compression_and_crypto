%! Author = tom
%! Date = 14/01/2021

% Preamble
\documentclass[a4paper, 11pt]{article}
\usepackage[margin=2cm]{geometry}
\title{Data Compression Report}
\author{}
\date{}

% Document
\begin{document}
\maketitle

My data compression implementation is predominantly based upon the statistical method of prediction by partial matching (PPM), used in conjunction with arithmetic coding. The encoder is essentially split into two distinct parts: an initial PPM step, deducing the probability that some input symbol $b$ is observed in a particular context $C$, and an arithmetic encoder which uses this probability to encode the symbol to the resulting output bit stream. Similarly, the decoder also implements both PPM and arithmetic decoding processes, although here the two are more closely intertwined. This report will focus on the description of the main concepts used in my compression (and decompression) process, explaining my method of PPM and arithmetic coding, and some of my personal adaptations.

PPM is an adaptive context-based technique, meaning the frequency assigned to each symbol in a particular context is continuously updated, converging on the optimal frequency distribution of the input text. Whilst being slower, this adaptive technique has certain benefits over static methods which rely upon constant frequency distributions with values based upon observed symbol frequencies from a collection of other sources. The main issue with this is its inability to effectively handle rare characters, in the extreme case assigning zero frequency a symbol which does not appear in the source texts (but may in our input text). This would be an issue for my encoder, which supports all symbols across the UTF-8 class --- some of which likely would not appear at all in many texts. Thus, I chose to explore adaptive modelling techniques, which address this by tailoring symbol frequency distributions specifically to each text. This also allows the compression to exploit nuances and pick up more novel frequency details in specific texts, improving  compression efficiency in instances which don't conform directly to a generalised static model. PPM stands out as the most appropriate method in this class as it quickly conforms to an efficient frequency distribution, negating the issue of initially inefficient compression (exhibited in most adaptive methods) by swiftly propagating frequency updates to all contexts longer of a higher order than that at which the symbol was encoded. Therefore, I selected PPM as the most suitable and advantageous model in this context.

For the PPM section of my encoder, the input \LaTeX\ file is processed incrementally as a stream of (UTF-8) bytes $b \in B$, sequentially considering contexts $c_{n} \in C$ with orders of size $n = N \to 0$ --- where maximum order was experimentally optimised to be $N = 6$ --- and searching the data structure $D$ for the context-byte pair $c_{n} \colon b$. Notably, for the first $N$ bytes of the input sequence we don’t have enough preceding bytes to construct full contexts for all orders $n$. Some implementations manoeuvre this by leaving the first $N$ symbols unencoded, however, this harms the compression ratio; thus, I chose to pad these contexts with common byte values such that these first few symbols are encoded, and the artificial byte sequence added to $D$ is beneficial in future.

For each context-byte pair $c_{n} \colon b$, the PPM step finds the probability interval within which it will be encoded by sequentially checking the value of $c_{n} \colon b$ against three criteria. Firstly, whether we have observed $c_{n}$ before, secondly if the sum over all the counts of symbols observed with this context in $D$ is non-zero --- i.e. $\sum_{b' \in D_{c_{n}}} count(c_{n} \colon b') > 0$ --- and lastly whether the search symbol $b$ has been observed (with non-zero count) in $c_{n}$. If any of these conditions fail ($c_{n}$ is new, $\sum_{b' \in D_{c_{n}}} count(c_{n} \colon b') = 0$, or $b$ is new/has zero count), we output the escape symbol $esc$ and either the interval $[0.0, 1.0)$ or the allocated non-zero interval of $esc$ in $c_{n}$. However, if all of these criteria hold, we have successfully found the input symbol $b$ and thus we output the associated probability interval of $b$ derived from the context table associated with $c_{n}$, and the PPM step is concluded.

The need to check both $c_{n}$ exists in $D$ and contains non-zero values derives from my implemented process of assigning the frequency of the escape symbol, known as \emph{PPMb}. Using this method, $esc$ gets assigned a frequency count equal to the number of other symbols $b'$ with non-zero count in $c_{n}$. To accommodate this count in the range of intervals, the counts of all $b'$s are decremented by one, which I have chosen to implement as all counts starting from 0 and $esc$ only being updated when the count of $b'$ is incremented to at least one. In order to deduce that this method of assigning escape frequency was the most applicable, I experimentally implemented a variety of other mechanisms, namely \emph{PPMa} in which the count is constantly 1, \emph{PPMc} where the count is (similarly) equal to the number of other symbols associated with $c_{n}$, and \emph{PPMa*} which searches for the shortest context with only one associated symbol $b'$ (a \emph{deterministic context}) from order 0 upwards, assigning input symbol $b$ this interval if $b' = b$ and resorting to PPMa otherwise. Through this experimentation I determined that PPMa* added unnecessary complexity to my implementation and only provided noticeable compression benefits for specific inputs. I also found that PPMb, whilst being slightly slower than PPMa and PPMc, outperformed both in a majority of instances, especially at the start of encodings when use of the escape symbol is common. When using short order contexts (which are less likely to be escaped), PPMa can outperform PPMb for highly repetitive texts as it assigns larger frequency intervals to each symbol (due to the smaller frequency of the escape symbol), however, in my particular use case and parameter setup I still found PPMb to produce more efficient encodings. Furthermore, PPMb has the effect that symbols are only counted when they are observed for the second time, and thus negating the need to assign a redundant interval to symbols which are unusually rare (only appear once in the input text), improving compression efficiency in a way that PPMa and PPMc do not take advantage of. 

Another extension to the basic form of PPM I implemented is what's known as the \emph{exclusion principle}. If we fail to locate the search symbol $b$ in context $c_{n}$, but its corresponding table does contain other symbols $b'$ with non-zero count, we can retain the knowledge into lower contexts that all symbols in the set of $b'$ are not our search symbol. Thus, we can exclude these symbols from all future computations (in the search for $b$). Namely, when we are in a lower context, items in this set are neglected from both the sum of values in that context and the search for an encoding interval. The use of an exclusion list noticeably improves the compression efficiency, as it means we can allocate a greater interval to $b$ when we locate it, as intervals which would be taken up by any $b'$ are discarded and their space redistributed.

Once we have our symbol (either $b$ or $esc$) and probability interval to encode --- generated from either PPM or a baseline frequency distribution used in order $-1$ --- it is passed into my arithmetic encoder for the generation of a compressed output bit stream. A standard arithmetic encoder uses a probability interval to encode a sequence of symbols almost optimally. The main advantage of using arithmetic encoding is its native ability to compress a sequence into a single binary codeword. This improves compression efficiency beyond other methods of statistical compression, such as Huffman coding which encodes each symbol as its own individual codeword, outputting a concatenation of these. It is possible to adapt Huffman coding to encode groups of symbols at a time, however it isn't very practical as to generate the binary code for a sequence of $m$ symbols, the codewords for all possible sequences length $m$ have to be generated. Therefore arithmetic coding, which natively utilises this grouping of symbols, is both more practical in this context and exhibits higher compression ratios, especially for alphabets where the frequency of different symbols is highly skewed (such as the UTF-8 character set I am using) and thus is most applicable for use in my implementation.

My encoder maintains an $m$-bit binary number for the low and high bounds of the encoding range, both being updated at each encoding step according to the low and high bound of the input probability interval. The low and high bounds then go through a process of `range adjusting', a technique whereby the resulting interval gets widened by shifting out bits of the binary representations of low and high. This works by repeatedly searching for the appearance of three cases: both most significant bits of low and high equal $0$ (meaning the interval's entirely in the range $[\frac{1}{2}max, max)$), both equal $1$ (the interval’s within $[min, \frac{1}{2}max)$), or they differ but the second most significant bit of low equals $1$ and high equals $0$ (the interval straddles the central range $[\frac{1}{4}max, \frac{3}{4}max)$). The interval is then adjusted accordingly to ensure it remains as wide as possible, generating the most efficient and accurate encoding. This encoding process is repeated for all input characters, generating the final output bit stream.

My decoder is similarly composed of both PPM and arithmetic coding parts, iteratively considering the full stream of encoded bits in chunks of length $m$, which it successively stores as the `tag'. For the PPM process, we sequentially consider contexts $c_{n}$ of order $n = N \to 0$, search for a byte to decode and update the tag and arithmetic decoding bounds, a distinct process in orders $n \geq 0$ and $-1$. I will start by addressing the latter, which is implemented as a regular arithmetic decoder, converting the integer value of the current tag (and the maximum frequency) to calculate the associated frequency value of the current decoding byte. We then search the baseline frequency distribution to find the interval within which this frequency falls, outputting the corresponding decoded byte $b$, and using the located interval to update the values of low, high and the tag. In order $n \geq 0$, the decoder builds up contexts (from the output stream of decoded bytes) and searches its own data structure $D$ against the first two conditions, verifying $c_{n}$ exists with non-zero values in $D$. If both are satisfied, we decode $b$ and update the bounds in a similar way to the aforementioned arithmetic decoder, but with a few crucial differences: we use the context count sum (and tag value) to decode the symbol frequency, and the symbol interval is derived from the cumulative context counts, either outputting $esc$ or a decoded byte $b$. However, in divergence with the encoder we do not terminate the decoding step here, as once we have decoded $b$ (in order $n$) we must return to the data structure $D$ and perform an update incrementing the counts of all observed context-byte pairs $c_{n} \colon b$, backtracking through orders $n = n \to N$. The decoder then returns to decoding the next tag value.

A personal adaptation I have made to typical implementations of PPM and arithmetic coding is the structure of the baseline frequency distribution $F$ used to code newly observed symbols in order $-1$. The conventional version of this relies upon a uniform distribution over the whole character set (in my case all UTF-8 bytes), as we assume to know nothing of unobserved symbols. However, this is relatively inefficient as it results in every new symbol being allocated an equal frequency interval. This is especially detrimental when using PPMb, as each new frequent symbol gets encoded in order $-1$ twice. Therefore, instead of assuming no knowledge of unseen symbols, I instead exploited the fact that the range of UTF-8 symbols contain sections of characters which differ by an experimentally determinable noticeable margin; for example, the characters with byte values in the range 97--122 (lowercase letters) generally appear in text significantly more frequently than those in range 58--64 (symbols such as the question mark) or 127--160 (control characters). To exploit this knowledge, I divided the UTF-8 spectrum into 11 sections and built on top of the basic uniform distribution by scaling the value of symbols in each section based upon their relative frequencies via an exponential distribution. Each section was assigned a relative ‘frequency level’ $x = 1 \to 5$ (with $x=5$ referring to the most frequent symbols) and each level allocated a point along the positive exponential function $e^{c * x}$, where $c$ is a regulating constant --- experimentally determined to generate the most efficient results at $c = 0.55$. Thus, the baseline frequency distribution is implemented by scaling the frequency of each symbol $i$ according to its frequency level $x_{i}$, generating the values $F[i] = \lfloor e^{c * x_{i}} \rfloor$, which are then assembled into a cumulative frequency distribution.

The final adaptation I'm going to discuss surrounds how the counts associated with each context-byte pair $c_{n} \colon b$ in $D$ are incremented after we have performed each search. A basic approach would naively increment each count by one whenever we observe the corresponding pair. However, this treats all values of $c_{n} \colon b$ homogeneously throughout the coding process, reducing the efficiency of later rounds, as a count incremented in an early stage remains at this level for the entirety of the process, even if it is no longer relevant to the current part of the sequence. For example, if a character appears repeatedly at the start it will rapidly accumulate a high count, however, later in the input this character may rarely appear, and yet it still maintains its high count relative to other more recently seen characters, unnecessarily reducing the available probability interval to encode in. This impact can be reduced by scaling back the relative magnitude of counts which have not been updated recently, to ensure more recent context-byte pairs have a larger impact on the counts in $D$. My implementation simulates this characteristic by increasing the step size $s$ that we increment counts as we advance further through the coding process. This inflates $s$ proportionally to the number of observed bytes by incrementing $s$ every $u$ steps, where $u=25,000$ was chosen as a compromise to generate a noticeable improvement in compression ratio whilst limiting counts within a reasonable bound for large input texts.

Thus, my statistical compression method utilises the core benefits of typical techniques, namely PPM and arithmetic coding, whilst extending certain aspects in an attempt to improve efficiency by implementing a range of ideas such as the PPMb escape method, exclusion lists, an exponential baseline distribution and relative count scaling.

\nocite{*}
\bibliographystyle{unsrt}
\bibliography{bibliog}

\end{document}