%! Author = tom
%! Date = 14/01/2021

% Preamble
\documentclass[a4paper, 11pt]{article}
\usepackage[margin=2cm]{geometry}
\title{Data Compression Report}
\author{Tom Potter}
\date{\today}

% Document
\begin{document}
\maketitle
\section{one}

My data compression implementation is predominantly based upon the statistical method of prediction by partial matching (PPM), used in conjunction with arithmetic coding. The encoder is essentially split into two distinct parts: an initial PPM step, deducing the probability that some input symbol $b$ is observed in a particular context $C$, and an arithmetic encoder which uses this probability to encode the symbol to the resulting output bit stream. Similarly, the decoder also implements both PPM and arithmetic decoding processes, although here the two are more closely intertwined. This report will focus on the description of the main concepts used in my compression (and decompression) process, explaining my method of PPM and arithmetic coding, and some of my personal adaptations.

PPM is an adaptive context-based technique, meaning the frequency assigned to each symbol in a particular context is continuously updated, converging on the optimal frequency distribution of the input text. Whilst being slower, this adaptive technique has certain benefits over static methods which rely upon constant frequency distributions with values based upon observed symbol frequencies across a collection of other sources. The main issue with this is its inability to effectively handle rare characters, in the extreme case assigning zero frequency a symbol which does not appear in the source texts (but may in our input texts). This would be an issue for my encoder, which supports all symbols across the UTF-8 class --- some of which likely would not appear at all in many texts. Thus, I chose to explore adaptive modelling techniques, which address this by tailoring symbol frequency distributions specifically to each text. This also allows the compression to exploit nuances and pick up more novel frequency details in specific texts, improving  compression efficiency in instances which don't conform directly to a generalised static model. PPM stands out as the most appropriate method in this class as it quickly conforms to an efficient frequency distribution, negating the issue of initially inefficient compression (exhibited in most adaptive methods) by swiftly propagating frequency updates to all contexts longer of a higher order than that at which the symbol was encoded. Therefore, I selected PPM as the most suitable and advantageous model in this context.

When it comes to the implementation of PPM in my encoder, the input \LaTeX\ file is considered incrementally as a stream of (UTF-8) bytes $b \in B$, sequentially considering contexts $c_{n} \in C$ with orders of size $n = N \to 0$ --- where maximum order was experimentally optimised to be $N = 6$ --- and searching the data structure $D$ for the context-byte pair $c_{n} \colon b$. Notably, for the first $N$ bytes of the input sequence we don’t have enough preceding bytes to construct full contexts for all orders $n$. Some implementations manoeuvre this by leaving the first $N$ symbols unencoded, however, this harms the compression ratio; thus, I chose to pad these contexts with common byte values such that these first few symbols are encoded, and the artificial byte sequence added to $D$ is beneficial in future (as the manufactured contexts are likely to be observed again).

Once we have the context-byte pair $c_{n} \colon b$, the PPM step finds the probability interval within which it will be encoded by sequentially checking the value of $c_{n} \colon b$ against three criteria. Firstly, whether we have observed $c_{n}$ before, secondly if the sum over all the counts of symbols observed with this context in $D$ is non-zero --- i.e. $\sum_{b' \in D_{c_{n}}} count(c_{n} \colon b') > 0$ --- and lastly whether the search symbol $b$ has been observed (with non-zero count) in $c_{n}$. If any of these conditions fails ($c_{n}$ is new, $\sum_{b' \in D_{c_{n}}} count(c_{n} \colon b') = 0$, or $b$ is new/has zero count), we output the escape symbol $esc$ and either the interval $[0.0, 1.0)$ or the allocated non-zero interval of $esc$ in $c_{n}$. However, if all of these criteria hold, we have successfully found the input symbol $b$ and thus we output the associated probability interval of $b$ derived from the context table associated with $c_{n}$, and the PPM step is concluded.

The need to check both $c_{n}$ exists in $D$ and contains non-zero values derives from my implemented process of assigning the frequency of the escape symbol, known as \emph{PPMb}. Using this method, $esc$ gets assigned a frequency count equal to the number of other symbols $b'$ with non-zero count in $c_{n}$. To accommodate this count in the range of intervals, the counts of all $b'$s are decremented by one, which I have chosen to implement as all counts starting from 0 and $esc$ only being updated when the count of $b'$ is incremented to at least one. In order to deduce that this method of assigning escape frequency was the most applicable, I experimentally implemented a variety of other mechanisms, namely \emph{PPMa} in which the count is constantly 1, \emph{PPMc} where the count is (similarly) equal to the number of other symbols associated with $c_{n}$, and \emph{PPMa*} which searches for the shortest context with only one associated symbol $b'$ (a \emph{deterministic context}) from order 0 upwards, assigning input symbol $b$ this interval if $b' = b$ and resorting to PPMa otherwise. Through this experimentation I determined that PPMa* added unnecessary complexity to my implementation and only provided noticeable compression benefits for significantly long inputs. I also found that PPMb, whilst being slightly slower than PPMa and PPMc, outperformed both in a majority of instances, especially at the start of encodings when use of the escape symbol is common. When using short order contexts (which are less likely to be escaped), PPMa can outperform PPMb for highly repetitive texts as it assigns larger frequency intervals to each symbol (due to the smaller frequency of the escape symbol), however, in my particular use case and parameter setup I still found PPMb to produce more efficient encodings on my tests. Furthermore, PPMb has the effect that symbols are only counted when they are observed for the second time, and thus negating the need to assign a redundant interval to symbols which are unusually rare (only appear once in the input text), improving compression efficiency in a way that PPMa and PPMc do not take advantage of.

Another extension to the basic form of PPM I implemented is what's known as the \emph{exclusion principle}. If we fail to locate the search symbol $b$ in context $c_{n}$, but its corresponding table does contain other symbols $b'$ with non-zero count, we can retain the knowledge into lower contexts that all symbols in the set of $b'$ are not our search symbol. Thus, we can exclude these symbols from all future computations (in the search for $b$). Namely, when we are in a lower context, items in this set are neglected from both the sum of values in that context and the search for an encoding interval. The use of an exclusion list noticeably improves the compression efficiency when compared to the standard implementation, as it means we can allocate a greater interval to $b$ when we locate it, as intervals which would be taken up by any $b'$ are discarded and their space redistributed.

Once we have our symbol (either $b$ or $esc$) and probability interval to encode --- generated from either PPM or a baseline frequency distribution used in order -1 --- it is passed into an my implementation of an arithmetic encoder for the generation of a compressed output bit stream. A standard arithmetic encoder uses a probability interval to encode a sequence of symbols almost optimally. The main advantage of using arithmetic encoding is its native ability to compress a sequence into a single binary codeword. This makes it more efficient at compressing than other methods of statistical compression, such as Huffman coding which (in its standard form) encodes each symbol as its own individual codeword, outputting a concatenation of these. It is possible to adapt Huffman coding to encode groups of symbols at a time, however it isn't very practical as to generate the binary code for a sequence of $m$ symbols, the codewords for all possible sequences length $m$ have to be generated. Therefore arithmetic coding, which natively exhibits this grouping of symbols, is both more practical in this context and exhibits higher compression ratios, especially for alphabets where the frequency of different symbols is highly skewed (such as the UTF-8 character set I am utilising) and thus is most applicable for use in my implementation.

My encoder maintains an $m$-bit binary number for the low and high bounds of the encoding range, both being updated at each encoding step according to the low and high bound of the input probability interval. The low and high bounds then go through a process of `range adjusting' a technique whereby the resulting interval gets widened by shifting out bits of the binary representations of low and high. This works by repeatedly searching for the appearance of three conditions: both most significant bits of low and high equal $0$ (meaning the interval's entirely in the range $[0.5, 1.0)$), both equal $1$ (the interval’s within $[0.0, 0.5)$), or they differ but the second most significant bit of low equals $1$ and high equals $0$ (the interval straddles the central range $[0.25, 0.75)$). Looping through this process ensures that the arithmetic encoding interval stays as wide as possible, ensuring the most efficient and accurate encoding. This encoding is repeated for all input characters, generating the final output bit stream.

My decoder is similarly composed of both PPM and arithmetic coding parts, and considers the full stream of encoded bits in chunks of length $m$ at a time, which it successively stores as the `tag'. For the PPM process, we sequentially consider contexts $c_{n}$ of order $n = N \to 0$, and the tag and low and high arithmetic decoding bounds are updated, a process different in orders $n \geq 0$ and $-1$. I will start by addressing the latter, which is implemented as a regular arithmetic decoder, converting the current tag to an integer and using it (and the cumulative maximum frequency) to calculate the associated frequency value of the current decoding byte. We then search the baseline frequency distribution to find the interval within which this frequency falls, output the corresponding decoded byte $b$, and use the located interval to update (and range adjust) the values of low, high and the tag. In order $n \geq 0$, the decoder builds up contexts (from the output stream of decoded bytes) and searches its own data structure $D$ against the first two conditions (verifying $c_{n}$ exists with non-zero values in $D$). If both these conditions are met, we decode $b$ and update the bounds in a similar way to the aforementioned arithmetic decoder, but with a few crucial differences: we use the tag value and context count sum to decode the symbol frequency, and the symbol interval is derived from the cumulative context counts, either outputting $esc$ or a legal decoded byte $b$. However, in divergence with the encoder we do not terminate this decoding step here, as once we have decoded byte $b$ (in order $n$) we now need to return to the data structure $D$ and perform an update for all observed context-byte pairs $c_{n} \colon b$, backtracking through orders $n = n \to N$. The decoder then returns to decoding the next tag value.

A personal adaptation I have made to typical implementations of PPM and arithmetic coding is the structure of the baseline frequency distribution $F$ used to code newly observed symbols in order $-1$. The conventional, simplistic version of this is to implement a uniform distribution over the whole character set (in my case all UTF-8 bytes), as we assume to know nothing of unobserved symbols. However, this is relatively inefficient as it results in every new symbol being allocated an equal frequency interval. This is especially detrimental in the case of PPMb, as each new frequent symbol gets encoded in order $-1$ twice. Therefore, instead of assuming no knowledge of unseen symbols, I instead exploited the fact that the range of UTF-8 symbols contain sections of characters which differ by an experimentally determinable noticeable margin; for example, the characters with byte values in the range 97--122 (lowercase letters) generally appear in text significantly more frequently than those in range 58--64 (symbols such as the question mark) or 127--160 (control characters). To exploit this knowledge, I divided the UTF-8 spectrum into 11 sections and built on top of the basic uniform distribution by scaling the counts of symbols in each section based upon their relative frequencies via an exponential distribution. Each section was assigned a relative ‘frequency level’ $x = 1 \to 5$ (with $x=5$ referring to the most frequent symbols) and each level allocated a point along the positive exponential function $e^{s * x}$, where $s$ is a regulating constant --- experimentally determined to generate the most efficient results at $s = 0.67$. Thus, the baseline frequency distribution is implemented by scaling the frequency of each symbol $i$ according to its frequency level $x_{i}$, generating the values $F[i] = \lfloor e^{s * x_{i}} \rfloor$, which are then assembled into a cumulative frequency distribution.
    
    \section{two}
    There is currently a range of attack methods against the Data Encryption Standard (DES) which exploit its multiple vulnerabilities. These range from linear cryptanalysis which creates a linear approximation of the encryption process to find biases in plaintext-ciphertext combinations for varying keys, to the improved Davies' attack, which analyses the non-uniform distribution produced by substitution boxes during an analysis phase to drastically reduce the key search space. These approaches aim to reduce the complexity of cracking DES, however they are known to be impractical to implement due to their reliance on a large collection of known plaintexts and ciphertexts: $2^{43}$ for an accurate linear cryptanalysis approximation, and $2^{50}$ to achieve a $51\%$ success rate in the Davies' attack. This makes these attacks incredibly memory intensive, and can negate the reduction in DES evaluations by relying upon a (possibly slow and unreliable) oracle to generate plaintext-ciphertext pairs. Therefore, the most feasible attack on DES remains an exhaustive key search (brute-force), whereby we test all $2^{56}$ possible variations of the (64-bit) key against a single known plaintext $P$ and ciphertext $C^{*}$ to find the corresponding target key $K^{*}$ used in this encryption. Therefore, this is the approach that my implementation will focus on in the following description and evaluation.

In order to mount an effective brute-force attack on the DES key space, the implementation must optimise its two core elements (the cipher and cracker) to as many rapid DES evaluations as possible - where we encrypt the input plaintext $P$ using a given test key $K$ - in the shortest time-frame. The first element is the implementation of a DES cipher used to conduct each encoding $E_{K}(P) = C'$. To maximise the throughput of the cipher, the pre-computation step (i.e. the computation of subkeys $k$ from $K$, and the application of the initial bit permutation $IP$), 16 distinct rounds of the DES Feistel cipher, and post-computation step (swapping the left and right blocks, and applying the final bit permutation $FP=IP^{-1}$), can be split up into distinct functions (allocated to different registers) which each form part of a full composite pipeline of the entire encryption process. The path of single test key $K'$ thus starts with the input of $K'$ and $P$ into the pipeline, where step 0 (pre-computation) is applied, generating the initial output $(L_{0}, R_{0})$ and a set of subkeys which are stored in a global $16 \times 16$ matrix, a single row holding all values of $k$ such that each column indicates the subkey $k_i$ used in DES round $i$. The 1st-16th step of this process then implement the sequential rounds of DES, with each round $i$ taking input of subkey $k_i$ and the output of the last round $(L_{i-1}, R_{i-1})$. The final round (post-computation) subsequently outputs the corresponding ciphertext $C'$, which we can quickly evaluate against the goal ciphertext $C^{*}$, outputting the found key $K' = K^{*}$ if $C' = C^{*}$.

Furthermore, as in this context we have access to an oracle returning the corresponding ciphertext to any plaintext (formatted as a hexadecimal string with length a multiple of 8), we can also ensure our value of $P$ to be optimally fast to encrypt. Therefore, we pick $P$ (and produce its corresponding $C^{*}$) such that it only contains 8 hexadecimal characters, and thus only requires one pass through the pipeline per encryption (whereas we would have to use more resources to encrypt the multiple blocks present in longer messages in ECB mode).

The full capacity of this pipeline can then be utilised by not only performing a single encoding $E_{K'_{j}}(P)$, but instead entering a new pair $(P, K'_{j+1})$ into the pipeline as soon as the previous encryption $E_{K'_{j}}(P)$ has completed its initial step (and stored its subkeys in row $j \pmod{16}$ of the subkey matrix), maximising the rate at which keys can be tested. This can be implemented by maintaining a constant global clock, whereby at each clock tick a new encryption enters the pipeline and all ongoing internal encryptions advance through another round of the cipher. Thus, there are 18 encryptions taking place simultaneously in the pipeline, with one encryption completing at each clock tick. We can then increase the frequency of the clock as high as possible, maximising the throughput of the pipeline whilst ensuring that each step is completed at every stage.

The other main element of the attack is the cracking process itself through which we take input of our test pair $(P, C^{*})$ and try all $2^{56}$ possible 56-bit values of $K'$ until one satisfies $E_{K'}(P) = C^{*}$. Our implementation also takex input of an integer $t$ specifying the number of available threads/compute nodes upon which we can run our attack in parallel. The cracker initially creates an array of length $t$, where each element $i$ specifies the start key ($\approx \frac{i*2^{56}}{t}$th possible key from the all-zero start key) and maximum number of evaluations ($\approx \frac{2^{56}}{t}$) such that each thread has an evenly balanced, non-overlapping interval of the key search space. We then initialise $t$ parallel cracking threads, where each thread $i$ operates on a separate domain, taking input of $(P, C^{*})$, the start key and number of evaluations assigned to $i$. Each thread iterates through its allocated interval of string keys, utilising the aforementioned DES pipeline to iteratively encrypt $P$ using each key $K'_{j}$. The process of checking outputs of the pipeline can further be parallelised by not checking for equality between the output ciphertext $C'_{j}$ and goal $C^{*}$ in the pipeline, or even in thread $i$ itself. Instead, we employ a hierarchical system where on the lower level are the cracking threads, and on a higher level are a set of `master' threads into which the pipeline output $C'_{j}$ is fed after each clock tick, allowing thread $i$ to immediately return to processing encryptions in its pipeline, and leaving the evaluation of whether $C'_{j} = C^{*}$ to the master thread, which can compute multiple of these computationally inexpensive comparisons in one step. The number of master threads $m \geq 1$ can be optimised to take maximal usage of the resources available, where in the case of multiple master threads ($m>1$) each evaluate the outputs of a distinct group of $\approx \frac{t}{m}$ cracker threads. If a master thread identifies that a key $K'$ (from any cracker thread) satisfies the evaluation, it sends a signal to close and join all threads throughout the system whatever state they are currently in (returning to the main thread) and outputs the value of the found key $K' = K^{*}$. If the threads are closed for any other reason, for example an error, the current key furthest through the encryption pipeline for each thread should be output and stored, such that the key search can be resumed again by setting those keys as the start key for each thread, and the adjusting number of evaluations accordingly.

As with all brute-force attacks, the time taken $T$ to find the target key $K^{*}$ is proportional to the number of bits in the key. In the case of DES, the 64-bit key has an effective size of 56 bits (as 8 are used as odd parity bits), and thus we have $T \propto 56$. As previously stated, there is precisely $2^{56}$ different combinations of these bits, meaning to find $K^{*}$ with 100\% certainty we would have to try all  $2^{56}$ variations of the key value. However, to give a majority success rate of 51\% we can assume that we need to test around $2^{55}$ values (half of the key space). It is clear that the speed at which this key space can be searched depends upon both the number of computing nodes (threads) and the speed at which these compute. For each instance of the DES pipeline, we complete one DES evaluation per clock tick - including the comparison against the target ciphertext (which we model as happening simultaneously due to multiple comparisons happening in parallel to each evaluation). Therefore, the speed at which we can make these evaluations is directly reliant upon the frequency $F$ of our implementation of the clock, which is a variable parameter given the resources of the machine being used. The number of parallel cracking threads $t$ (and master threads $m$) can also be adjusted in line with the available resources (maximising the total $t+m$ without overflowing the available compute/memory space), theoretically improving the time to find $K^{*}$ from $2^{55}$ clock ticks to $\frac{2^{55}}{t}$. Thus, the time in seconds is given by $(\frac{2^{55}}{t})/F = \frac{2^{55}}{t*F}$, where $t$ is an integer and $F$ is measured in \emph{Hz}. Note that the time taken to compute $C' = C^{*}$ for a single ciphertext is negligible compared to the necessary time to compute a single round of DES, and thus we can implement $m \gg t$; therefore, in subsequent calculations I will simply refer to $t$. For example, on a state-of-the-art machine, our aim for optimal $t$ would be around 1800 (the number of parallel chips used in the EFF implementation ``Deep Crack" in 1998), paired with a desirable but reasonable frequency $F$ of around 50MHz, which would allow us to evaluate $t * F = 1800 * \num{5e7} = \num{9e10}$ keys per second, and thus it would take $\frac{2^{55}}{\num{9e10}} = 400320$ seconds, or approximately 4.6 days. An even more powerful setup, such as that used to crack DES by D. Hulton and M. Marlinspike in 2012, would have $t = 1900$ and $F = \num{4e8}$\emph{Hz}, which using my evaluation would take $\frac{2^{55}}{1900 * \num{4e8}} = 47406$ seconds, or approximately 13 hours. However, it is not given that this frequency value would be reachable, as the maximum frequency depends upon the resources of the machine being used and thus should be determined experimentally after the implementation has been built.
    
\end{document}